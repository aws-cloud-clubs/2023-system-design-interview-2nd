# 안정 해시 설계

수평적 규모 확장성을 달성하기 위해서는 요청 또는 데이터를 서버에 균등하게 나누는 것이 중요하다.

## 해시 키 재배치(rehash) 문제

N개의 캐시 서버에 부하 균등 하게 나누는 방법 -> serverIndex = hash(key) % N (N은 서버 갯수)

해당 방법은 서버 풀(server pool)의 크기가 고정되어 있을 때, 그리고 데이터 분포가 균등할 때는 잘 작동한다.
하지만 서버가 추가되거나 기존 서버가 삭제되면 문제가 발생한다.
가령 4개의 서버가 존재하다가 1번 서버가 장애가 나서 동작이 중단되면 서버풀의 크기는 3으로 바뀐다
그 결과, 키에 대한 해시 값은 변하지 않지만 나머지(%) 연산을 적용하여 계산한 서버 인덱스 값을 달라질 것이다.
즉, 나누는 몫이 달라져 결과로 나오는 서버 인덱스 값이 해시값마다 다 달라지게 된다.

## 안정 해시

안정 해시(consistent hash)는 해시 테이블 크기가 조정 될 때 평균적으로 오직 k/n개의 키만 재배치하는 해기 기술.
여기서 k는 키의 갯수이고, n은 슬롯(slot)의 갯수다.

### 해시 공간과 해시 링

해시 함수 f로는 SHA-1을 사용한다고 하고, 그 함수의 출력 값 범위는 x0,x1,x2,x3...xn 과 같다.
SHA-1의 해시 공간 범위는 0부터 2^160-1 까지라고 알려져 있다.
따라서 x0는 0, xn은 2^160-1이다.

이렇게 일렬로 세워놓은 해시 공간의 끝과 끝을 이어붙인 것이 해시 링(hash ring)이다.

### 해시 서버

이 해시 함수 f를 사용하면 서버IP나 이름을 이 링 위의 어떤 위치에 대응시킬 수 있다.
( s0 = 서버0, s1=서버1, s2=서버2, s3=서버3)

### 해시 키

캐시할 키 key0, key1, key2, key3는 해시 링 위의 어느 지점에 배치 할 수 있다.

### 서버 조회

어떤 키가 저장되는 서버는, 해당 키의 위치로부터 시계 방향으로 링을 탐색해 나가면서 만나는 첫 번째 서버다.

key0은 서버 0에 저장되고 key1은 서버 1에 저장되며, key2는 서버 2, key3는 서버 3에 저장된다.

### 서버 추가

따라서, 서버가 추가되더라도 키 가운데 일부만 재배치하면 된다.
가령, 해시 링 위에 s0,s1,s2,s3가 원을 그리며 존재하고 key0이 s0과 s3사이에 존재한다고 하자. 거기에 s4가 s3,s0에 새로 증설되었다고 하자.
그러면 서버가 추가되기 전, key0은 서버4에 저장될 것인데 왜냐하면 key0의 위치에서 시계 방향으로 순회했을 때 처음으로 만나게 되는 서버가 서버4이기 때문이다.

### 서버 제거

하나의 서버가 제거되면 키 가운데 일부만 재배치된다.
서버1이 삭제되면 특정 키는 시계방향으로 순회공연을 돌면서 가장 먼저 만나게되는 서버에 저장된다.

## 기본 구현법의 두 가지 문제

안정 해시 알고리즘은 MIT에서 처음 제안되었다. 기본 절차는 다음과 같다.

- 서버와 키를 균등 분포 해시 함수를 사용해 해시 링에 배치한다.
- 키의 위치에서 링을 시계 방향으로 탐색하다 만나는 최초이 서버가 키가 저장될 서버다.

이 접근법에는 2가지 문제 존재

1. 서버가 추가되거나 삭제되는 상황 감안시, 파티션의 크기를 균등하게 유지하는 것이 불가능
   (특정 공간에는 엄청 널널하지만 특정 공간은 엄청 촘촘할 수 있음)
   여기서 파티션은 인접한 서버 사이의 해시 공간을 뜻함.

2. 키의 균등 분포를 달성하기가 어렵다.
   즉, 특정 서버에만 키가 몰릴 수 있는 가능성이 존재한다.

# 해결법

가상노드 사용 또는 복제 기법 사용

### 가상노드

가상 노드는 실제 노드 또는 서버를 가리키는 노드로, 하나의 서버는 링 위에 여러 개의 가상 노드를 가질 수 있다.

가령 서버0을 링 위에 배치하면서도 s0_0, s0_1, s0_2 ... 등 가상 노드를 추가 하는 것이다.

따라서 각 서버는 하나가 아닌 "여러 개"의 파티션을 관리해야 한다.

서버 0,1이 있을때 가상 노드의 배치는 s0_0, s0_1 s0_2 이런식으로 0번 서버의 노드만 일괄적으로 배치하는게 아닌 s0_0, s1_0 , s0_1, s1_1 식으로 엇갈리며 배치한다. (균등 배치를 위함)

가상 노드의 갯수를 늘리면 키의 분포는 점점 더 균등해진다 -> 표준 편차가 작아져 데이터가 고르게 분포됨.
표준편차는 데이터가 어떻게 퍼져 나갔는지를 보이는 척도다.

100-200개 가상 노드 사용시 : 표준 편차값은 평균의 5프로(200개)-10프로(100개) 사이
가상 노드 갯수를 더 늘릴수록 표준 편차 값은 더 떨어진다.

그러나 가상 노드 자체를 저장할 공간은 더 많이 필요하게 되므로 trade-off 계산을 잘 해야한다.

### 재배치할 키 결정

서버 증설/제거시 데이터 일부를 재배치하는 과정이 필요하다.
어느 범위의 키들이 재배치되어야 할까?

위에선 언급한 것처럼 가령 s0~s4 까지의 서버가 있다고 가정하고 s0 뒤로 s5가 추가되어 s0과 s4 사이에 존재한다고 하자.
그러면 기존에는 s0과 s4 사이에 존재하던 key0은 증설된 s5에 포함되며 이 말은 key0뿐만 아니라 새로 추가된 노드(s5) 부터 그 반시계 방향에 있는 첫번째 서버 s4 까지의 모든 키들이 s5에 포함되게 되는 것이다.

### 안정 해시 사용 이점

- 서버가 추가되거나 삭제될 때 재배치되는 키의 수가 최소화된다.
- 데이터가 보다 균등하게 분포하게 되므로 수평적 규모 확장성을 달성하기 쉽다.
- 핫스팟키 문제를 줄인다. 특정 샤드에 대한 접근이 지나치게 빈번하면 서버 과부하 문제가 생길 수 있다.
- 케이티페리, 레이디가가, 저스틴비버와 같은 유명인의 데이터가 전부 같은 샤드에 몰리는 상황을 생각하면 된다.
- 안정 해시는 데이터를 좀 더 균등하게 분배하므로 이런 문제가 생길 가능성을 줄인다.

#### 안정 해시 사용 예

- 아마존 DynamoDB의 파티셔닝 관련 컴포넌트
- 아파치 카산드라 클러스터에서의 데이터 파티셔닝
- 디스코드 채팅 어플리케이션

## 실제 사례 공부하기

ref : https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf

### Partitioning Algorithm - P209

- Dyanamo의 주요 설계 요구 사항 중 하나는 점진적인 확장성을 가져야 한다는 것이다.
- 이것은 데이터를 시스템의 노드 집합 (즉, 스토리지 호스트)에 동적으로 분할하는 메커니즘이 필요하다.
- 다이마노의 파티셔닝 전략은 부하를 분산시키기 위해 해시 전략에 의존한다.
- 지속적인 해싱 전략 안에서, 해시 함수의 결과 범위는 고정된 순회적 공간 혹은 "링"이라는 것으로 간주된다.
- 시스템 안에 있는 각각의 노드는 무작위 값이 할당되며 그것의 링 안에서의 자기 고유의 위치값을 상징한다.
- 각 키로 식별된 데이터 항목은 해당 데이터 항목의 키를 해싱하여 그 위치를 찾아내고, 그런 다음 시계방향으로 링을 따라가면서 데이터의 항목보의 위치보다 큰 위치를 가진 "첫번째" 노드에 할당된다.

- 따라서 각 노드는 링 상에서 자신과 이전 노드의 사이의 영역에 대한 책임을 지게 됩니다.
- 일관된 해싱의 주요 장점은 노드의 추가 또는 제거가 해당 노드의 가장 근접한 노드와 다른 노드에게만 영향을 준다는 것이다.
- 기본적인 해싱 알고리즘은 다음과 같은 문제가 있습니다.

1. 링 상의 각 노드에 대한 임의의 위치 할당은 비균일한 데이터 및 부하 분배로 이어진다.
2. 기본 알고리즘은 노드의 성능 다양성을 간과한다.

따라서, 다이나모는 가상 노드의 개념을 사용한다.

가상노드는 시스템 내의 단일 노드처럼 보이지만 각 노드는 하나 이상의 가상 노드에 대해 책임을 부여 받습니다.
시스템에 새로운 노드가 추가되면 해당 노드는 링 내에서 여러 위치 ( = 토큰이라고함) 를 할당는다.

#### 균일한 부하 분배 보장하기 (토큰 => '위치', 파티션 => '공간')

- Dyanmo에서 관찰된 부하 불균형과 부하 분배에 대한 다양한 분할 전략 영향에 대해서 언급할 것이다.
- 부하 불균형 및 요청 부하와의 상관관계를 연구하기 위해 각 노드가 30분 간격으로 측정된 24시간 동안 받은 총 요청수를 측정.
- 주어진 시간창에서 노드는 노드의 요청 부하가 평균 부하에서 특정 임계값 "미만"으로 나타나면 "블균형"으로 간주. (여기서는 15프로)
- 해당 기간동안 전체 시스템이 받은 요청 부하도 함께 기록.
- 불균형 비율은 부하 증가에 따라 감소 -> 즉, 균형적으로 부하가 분산되고 있다는 말.
- 예를들어 부하가 낮은 경우 불균형 비율이 20프로 정도로 높고 부하가 높은 경우 10프로에 가까워짐.
- 이 말은 즉슨, 저스틴 비버를 많이 찾는 검색 쿼리에 대한 부하가 전체적으로 각각의 노드에 잘 분산되고 있다는 증거.
- 그러나 낮은 부하( 최대부하의 1/8 정도) 는 저스틴 비버의 키가 더 적게 엑세스 되므로 부하 불균형이 더 높아짐.

<b><전략1></b>
초기에는 노드당 T개의 무작위 토큰값으로 분할을 하였음 -> 초기 프로덕션 전략. 각 노드에 T개의 토큰이 할당됨 (해시 공간에서 노드의 공간이 무작위로 할당)

- 문제점 : 새로운 노드가 참여하면 다른 노드가 가지고 있는 키의 범위값을 알아야함. 그러나 키 범위를 새로운 노드에게 전달하는 과정이 리소스 집약적인 작업이기에 고객 만족을 떨어뜨리는 요인이됨. 또한 둘째로 노드가 시스템에 참여 혹은 제거됬을 때 많은 노드가 참여하는 키의 범위가 변경되고 새로운 범위에 대한 트리를 다시 계산해야 하는데 이는 프로덕션 시스템에서 처리하기 어려움. 마지막으로 키 범위의 무작위성으로 인해 전체 키 공간의 스냅샷을 쉽게 가져오기가 어려움.

해당 전략의 근본적인 문제는 데이터 분할 및 데이터 배치 방식에 대한 방식이 서로 얽혀 있다는 것이다.
예를들어 요청 부하 증가를 처리하기 위해 시스템에 더 많은 노드를 추가하게되면 반드시 필수불가결하게 데이터를 분산시키는 작업이 요구된다는 것이다.

따라서 이상적으로는 분할/배치의 작업이 독립적으로 이루어져야 바람직하기에 다음과 같은 전략으로 수정.

<b><전략2></b> - P216
해시 공간을 동일한 크기의 파티션/범위로 나누고 각 노드에 T개의 무작위 토큰 할당.
이 전략에서는 토큰은 해시 공간의 값들을 노드 목록으로 매핑하는 함수를 구축하는만 데만 사용되며 분할을 결정하지 않고 노드의 순서대로 나열 하는 것. A파티션은 처음에 만나는 N개의 고유 노드에 배치됨. 이 전략의 주요 장점은 분할과 파티션 배치의 분리 및 실행 중에 배치 방식을 변경할 수 있는 여지가 존재한다는 것.

--> 다만, 노드들간의 위치가 고르게 분포되지 않음 (파티션만 고르게 분포)

<b><전략3></b>
각 노드당 동일한 토큰 및 동일한 크기의 파티션을 제공하는 전략. 전략2와 유사하게 이 전략은 해시 공간을 동일한 크기의 Q 파티션으로 나누고 파티션의 배치가 분할 방식과 분리된다. 또한 각 노드에는 시스템의 노드수를 파티션으로 나눈 균일한 토큰이 할당된다. (쉽게 말해, 각 노드마다 균일한 공간이 배정된다는 뜻. 한쪽으로 치우치지 않고) 노드가 시스템에서 제거된다면 해당 노드가 가지고 있는 토큰은 이러한 속성(균일 배정)을 보존하도록 다른 노드에 분산됨.

같은 논리로 특정 노드가 시스템에 추가되면 해당 속성을 보존하기 위해 시스템 내의 노드로부터 토큰을 훔쳐받게됨

이러한 다른 전략을 공정하게 비교하는 것은 어렵습니다. 다른 전략은 효율성을 조정하기 위한 다른 구성을 가지고 있기 때문입니다. 예를 들어, 전략 1의 부하 분산 속성은 토큰 수에 의존하고 전략 3은 파티션 수에 의존합니다. 이러한 전략을 비교하는 공정한 방법은 모든 전략이 회원 정보를 유지하기 위해 동일한 양의 공간을 사용하는 동안 부하 분산에서의 차이를 평가하는 것입니다. 예를 들어, 전략 1에서 각 노드는 링 내의 모든 노드의 토큰 위치를 유지해야 하며, 전략 3에서 각 노드는 각 노드에 할당된 파티션에 대한 정보를 유지해야 합니다.

--> 파티션도 고르게 분포 노드의 위치도 균형적

---

따라서 다음 실험에서는 이러한 전략들이 관련 매개변수를 변화시킴으로써 평가되었다. 각 전략의 부하 분산 효율성은 각 노드에서 유지해야하는 회원정보의 크기에 따라 측정되었다. 여기서의 부하 분산 효율성은 각 노드에서 서비스하는 평균 요청 수와 가장 핫한 노드에서 서비스하는 최대 요청 수의 비율로 정의됨.

P217 -> 그림8에 표시된 결과를 통해 확인하면 전략3은 가장 효율적인 부하 분산 효율성을 달성하고 전략2는 가장 나쁜 부하 분산 효율성을 갖게됨. 전략3은 전략1과 비교하여 더 나은 효율성을 제공하며 각 노드에서 유지되는 멤버십 정보의 크기를 3개의 크기 순서대로 줄입니다.

저장용량이 큰 문제는 아니지만 노드는 노드간에 주기적으로 정보를 뺏고 뺏기므로 압축하는 방식이 바람직하다.

또한 다음과 같은 이점 존재

1. 빠른 부트스프래핑 / 복구 : 파티션 범위가 고정되어 있기에 파티션은 특정 파일의 형태 + 고정된 형태로 저장이 가능해지며 단순히 파일을 전송해 파일을 파티션 단위로 재배치 할 수 있음. ( 특정 항목을 찾기 위한 랜덤 접근 안해도됨)

2. 아카이빙 용이성 : Dynamo에서 저장된 데이터 세트의 주기적인 아카이빙은 대부분의 Amazon 저장 서비스에 필수적인 요구 사항입니다. 전략 3에서는 파티션 파일을 별도로 아카이빙하기 때문에 Dynamo이 저장한 전체 데이터 세트를 아카이빙하는 것이 더 간단

이와는 달리 Strategy 1에서는 토큰이 무작위로 선택되며, Dynamo에 저장된 데이터를 아카이빙하려면 개별 노드에서 키를
검색해야 하므로 효율적이고 빠르지 않을 경우가 많음.

### 정리

기본적으로 안정 해시를 구현하기 위해서는 해시 링안에 서버가 고르게 분포되어 있어야 하며 (고르게 분포되기 위해서는 링을 균일하게 partitioning 해줘야함) 따라서 균일하게 파티션을 유지하기 위해서 하나의 서버가 하나의 파티션을 관리하는 것이 아니라 가상노드를 도입해 각 서버는 자식 노드들을 달아서 여러개의 파티션들을 관리한다 (ex: s0_0, s0_1, s0_2 ...)
이렇게 가상노드를 늘려주게 되면 서버간의 거리차이 즉 표준편차가 줄어들어 데이터가 균등하게 분포될 수 있다.
(하지만 가상노드를 늘려주기 위한 데이터 공간이 요구되는것 역시 고려해야 할 사안)

중요 변수 : 데이터 양 자체, 파티셔닝 갯수, 서버 갯수
