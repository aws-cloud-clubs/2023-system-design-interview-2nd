### URL 단축

단축 URL을 [https://tinyurl.com/{hashValue}](https://tinyurl.com/{hashValue}) 라고 한다면, 중요한 것은 긴 URL을 hashValue에 대응시키는 hash function을 찾는 것이다.

<img src="https://velog.velcdn.com/images/kshired/post/e32ed9b7-30a9-4ee0-914b-d5e15a5a5ab7/image.png">

해시 함수는 다음 요구사항을 만족해야 한다.

- 입력으로 주어지는 긴 URL이 다른 값이면 해시 값도 달라져야 한다.
- 계산된 해시값은 원래 입력으로 주어졌던 긴 url로 복원될 수 있어야 한다.

### [상세 설계]

#### 데이터 모델

해시테이블에 저장하기에는 문제가 있다. 그렇기에 < 단축 URL, 원래 URL > 을 관계형 데이터베이스에 저장하는 것이 나을 것이다.

<img src="https://velog.velcdn.com/images/kshired/post/9e50a7e5-4ed3-44cd-a772-4cfe73063d26/image.png">

- 간단하게 id, shortURL, longURL 을 저장하는 url table을 설계했다.

#### 해시 함수

해시 함수는 원래 url을 단축 url로 변환하는데 쓰인다.

<b> 해시 값 길이 </b>

- hashValue는 [0-9, a-z, A-Z]의 문자들로 구성되며, 사용할 수 있는 문자의 개수는 62개이다.
- hashValue를 정하기 위해, 62^n ≥ 3650억인 n의 최소값을 정해야한다.
- n = 7 이면, 3.5조개의 URL을 만들 수 있기 때문에 hashValue를 7로 두자.

<b> 해시 후 충돌 해소 </b>

긴 URL을 줄이려면, 원래 URL을 7글자로 줄이는 해시함수가 필요하다.

- 자주 사용되는 해시함수는 보통 아래와 같다.

<img src="https://velog.velcdn.com/images/kshired/post/218e1c7d-6b83-47a9-8d25-411e3865e271/image.png">

하지만 위의 방법으로는 가장 짧은 해시값조차도 7보다는 길다. 어떻게 하면 줄일 수 있을까?
첫번째 방법은 계산된 해시 값에서 처음 7개 글자만 이용하는 것이다.

하지만 충돌 확률 이 높아지기 때문에, 충돌이 발생했을 때 사전에 정한 문자열을 해시값에 덧붙이는 방식을 사용한다.

<img src="https://velog.velcdn.com/images/kshired/post/71253e52-0c28-4919-b3ff-872595f3cd4e/image.png">

위의 방법을 사용하면 충돌은 해소 가능하지만 단축 url을 생성할 때 한 번 이상 데이터베이스 질의를 해야 하므로 오버헤드가 크다.
데이터베이스 대신 블룸 필터를 사용하면 성능을 높일 수 있다. 블룸 필터는 어떤 집합에 특정 원소가 있는지 검사할 수 있도록 하는 확률론에 기초한 공간 효율이 좋은 기술이다.

<b> base-62 변환 </b>

진법 변환은 URL 단축기를 구현할 때 흔히 사용되는 접근법 중 하나.
이 기법은 수의 표현 방식이 다른 두 시스템이 같은 수를 공유하여야 하는 경우에 유용.
62진법을 쓰는 이유는 hashValue에 사용할 수 있는 문자 개수가 62개이기 때문이다.

62진법은 수를 표현하기 위해 총 62개의 문자를 사용하는 진법이다. 따라서 0은 0으로, 9는 9로, 10은 a로 … 61은 Z로 표현하도록 할 것임.

EX) ‘a’ = 1010, ‘Z’ = 6110

11157 =2x62^2 + 55x62^1 + 59x62^0 =[2,55,59]->[2,T,X] 이다.

<img src="https://velog.velcdn.com/images/kshired/post/62feec82-e648-43e5-9990-02103b50c96b/image.png">

위 방식에 따라, 단축 URL은 [https://tinyurl.com/2TX](https://tinyurl.com/2TX) 가 된다.

<b> URL 단축기 상세 설계 </b>

62 진법을 통해 단축기 상세 설계를 진행해보면, 아래와 같다.

<img src="https://velog.velcdn.com/images/kshired/post/b17ea4bc-f0d7-4eb7-b566-0b21e94e1fca/image.png">

1. 입력으로 긴 url을 받는다.
2. 데이터베이스에 해당 url이 있는지 검사한다.
3. 있다면, URL을 데이터베이스에서 가져와 클라이언트에 반환한다.
4. 없으면 유일한 ID를 생성한다. 이 ID를 데이터베이스의 기본 키로 사용한다.
5. 62진법 변환을 적용, ID를 단축 url로 만든다.
6. ID, 단축 URL, 원래 URL을 데이터베이스의 새 레코드로 만든 후 단축 URL을 클라이언트에 전달한다.

아래의 예시가 생성되는 데이터베이스의 새 레코드이다.

<img src="https://velog.velcdn.com/images/kshired/post/00e448fd-80b3-4790-b21c-de21232dd6a8/image.png">

### 리디렉션 상세 설계

<img src="https://velog.velcdn.com/images/kshired/post/875d27f0-7394-4259-b212-72f13e6a6c6d/image.png">

쓰기보다 읽기를 더 자주하는 시스템이기 때문에, < 단축 URL, 원래 URL > 을 쌍으로 캐시에 저장하여 성능을 높였다.

로드 밸런스의 동작은 아래와 같이 요약가능하다.

1. 사용자가 단축 URL을 클릭한다.
2. 로드밸런서가 해당 클릭으로 발생한 요청을 웹 서버에 전달한다.
3. 캐시에 있는 경우, 캐시에 바로 꺼내서 클라이언트에 전달한다.
4. 없는 경우 데이터베이스에서 꺼낸다. 만약에 데이터베이스에 전달하지 않는다면, 사용자가 잘못된 URL을 입력한 경우일 것이다.
5. 데이터베이스에서 꺼낸 URL을 캐시에 저장하고, 사용자에게 전달한다.

### 추가공부

ref : https://discord.com/blog/how-discord-stores-trillions-of-messages

기존의 디스코드 (2017년 이전)는 카산드라 디비 사용해서 메시지 저장.
12개의 노드 사용

2022년에는 177개의 노드에 수조 개의 메시지가 저장되어 있다.

```java

CREATE TABLE messages (
   channel_id bigint,
   bucket int,
   message_id bigint,
   author_id bigint,
   content text,
   PRIMARY KEY ((channel_id, bucket), message_id)
) WITH CLUSTERING ORDER BY (message_id DESC);
```

위의 테이블 생성 DDL은 메시지 스키마의 최소 버전. channel_id는 스노우플레이크 기법 사용해 시간순으로 정렬
메시지를 구분하는 방법은 보내는 채널과 정적 시간대로 분류. 이러한 분류는 카산드라에 주어진 채널과 버킷에 대한 모든 메시지가 함께 저장되고 세 노드 (또는 replication node)에 걸쳐 복제됨.

이러한 파티셔닝에는 잠재적인 성능 문제가 존재하는데, 친구 그룹이 적은 서버는 수십만 명의 서버보다 훨씬 적은 수의 메시지를 보내는 경향.

카산드라에서 읽기는 쓰기보다 더 비싸다. 쓰기는 커밋 로그에 추가되고 메모리 내 구조인 멤테이블에 기록되며 결국 디스크로 플러시된다. 그러나 읽기는 멤테이블과 잠재적으로 여러 SSTables(디스크 파일)를 쿼리해야 하므로 더 비싼 작업입니다.

사용자가 서버와 상호 작용할 때 동시에 많이 읽히는 것은 "핫 파티션"이라고 합니다. 이러한 액세스 패턴과 결합하면 데이터셋의 크기가 클러스터에 어려움을 초래했습니다.

핫 파티션이 발생하면 전체 데이터베이스 클러스터의 지연 시간에 영향을 주는 경우가 많았습니다. 하나의 채널과 버킷 쌍은 많은 트래픽을 수신했으며 노드가 트래픽을 처리하기 위해 점점 더 노력하고 점점 더 뒤쳐지면서 노드의 지연 시간이 증가했습니다.

Cassandra를 사용할 때 핫 파티션에 대한 어려움을 겪었습니다. 특정 파티션으로의 높은 트래픽은 제한 없는 동시성을 초래하며, 이로 인해 후속 쿼리가 지연 시간이 계속해서 증가하는 카스케이딩 지연이 발생했습니다. 핫 파티션으로의 동시 트래픽 양을 제어할 수 있다면 데이터베이스가 과부하되는 것을 방지할 수 있을 것입니다.

이 작업을 수행하기 위해 우리는 <b>데이터 서비스</b>라고 하는 것을 작성했습니다. 이는 우리의 API <b>모노리스</b>와 데이터베이스 클러스터 사이에 위치하는 중간 서비스입니다. 데이터 서비스를 작성할 때 우리는 디스코드에서 점점 더 많이 사용하고 있는 언어를 선택했습니다. 그 언어는 러스트입니다! 이전에도 몇 가지 프로젝트에서 사용해보았고, 우리에게는 그 혹독한 속도를 제공하지만 안전성을 희생하지 않고 얻을 수 있었습니다.

#### 우리의 데이터 서비스는 API와 ScyllaDB 클러스터 사이에 위치합니다. 이 서비스에는 데이터베이스 쿼리당 대략 하나의 gRPC 엔드포인트가 있으며 의도적으로 비즈니스 로직은 포함되어 있지 않습니다. 데이터 서비스가 제공하는 큰 기능은 요청 병합입니다. 여러 사용자가 동시에 동일한 행을 요청하는 경우 데이터베이스를 한 번만 쿼리합니다. 첫 번째 사용자가 요청을 하면 서비스에 작업자 작업이 생성됩니다. 후속 요청은 해당 작업의 존재 여부를 확인하고 해당 작업에 구독합니다. 해당 작업자 작업은 데이터베이스를 쿼리하고 행을 모든 구독자에게 반환합니다.

<img src="https://assets-global.website-files.com/5f9072399b2640f14d6a2bf4/6406629e7ba3569d3c32c8ed_Example%201%402x.png">

@everyone에게 알려주는 대규모 서버에서 엄청난 알림이 발생했다고 생각해 봅시다. 사용자가 앱을 열고 메시지를 읽게 되어 엄청난 트래픽이 데이터베이스로 전송됩니다. 이전에는 핫 파티션이 발생할 수 있으며 시스템 복구를 위해 호출 시 호출이 필요할 수도 있습니다. 데이터 서비스를 통해 데이터베이스에 대한 트래픽 급증을 크게 줄일 수 있었습니다.

<img src="https://assets-global.website-files.com/5f9072399b2640f14d6a2bf4/640662a51e3e13599d292404_Example%202%402x.png">

두 번째 부분은 데이터 서비스의 업스트림입니다. 우리는 보다 효과적인 병합을 가능하게 하기 위해 데이터 서비스에 일관된 해시 기반 라우팅을 구현했습니다. 데이터 서비스에 대한 각 요청에 대해 라우팅 키를 제공합니다. 메시지의 경우 채널 ID이므로 동일한 채널에 대한 모든 요청은 동일한 서비스 인스턴스로 이동합니다. 이 라우팅은 데이터베이스의 부하를 줄이는 데 더 도움이 됩니다.
