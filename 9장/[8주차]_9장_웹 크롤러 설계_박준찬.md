# 구조

![image](https://github.com/junchanpp/2023-system-design-interview-2nd/assets/49396352/a5b752fe-b3f3-4ce2-be9f-af978bd41de7)


- 시작 Url 집합
    
    웹 크롤러가 크롤링을 시작하는 출발점. 도메인 자체가 될 수도 있고, 주제별로 다른 시작 URL을 가질 수도 있다.
    
- 미수집 URL 저장소
    
    다운로드할 URL과 다운로드된 URL를 두가지로 나눠서 관리. 이미 수집된 정보를 다시 수집한다면 효율성 문제가 생기고, 다운로드된 URL를 오랜 기간 갱신하지 않으면 정보의 최신성에 문제가 생김.
    
- HTML 다운로더, 도메인 이름 변환기
    
    HTML 다운로더는 웹 페이지를 다운로드하는 컴포넌트. 도메인 이름 변환기는 URL을 IP 주소로 변환하는 절차.
    
    다운로드를 받기 전에 로봇 제외 프로토콜을 유의해야 한다. Robots.txt는 웹사이트가 크롤러와 소통하는 표준적 방법이다. 이 파일은 크롤러가 어떤 페이지를 수집해도 되는지 목록이 적혀 있다. 따라서 웹 사이트를 크롤링하기 전에 크롤러는 해당 파일에 나열된 규칙을 먼저 확인해야 한다.
    
    도메인 이름 변환기의 결과를 캐싱해 둔다면 크롤링 성능을 향상 시킬 수 있다. DNS 요청을 보내고 결과를 받는 작업의 동기적 특성을 해소하기 위해 캐시를 해둬야 한다. 이 작업 동안 어느 한 스레드가 이 작업을 하고 있다면 다른 스레드의 DNS 요청은 모두 block된다.
    
- 콘텐츠 파서
    
    파싱과 검증 절차.
    
- 중복 컨텐츠인가?
    
    웹 페이지의 해시 값 비교로 확인(혹은 체크섬)
    
- 콘텐츠 저장소
    
    HTML 문서를 보관하는 시스템. 데이터 양이 너무 많을 때 대부분의 콘텐츠는 디스크에 저장하고, 인기 있는 콘텐츠는 메모리에 두어 접근 지연 시간을 줄이는 전략 선택.
    
- URL 추출기
    
    HTML을 파싱하여 링크들을 골라내는 역할
    
- URL 필터
    
    원하지 않는 URL이나, 접소시 오류가 발생하는 URL등을 크롤링 대상에서 배제한다.
    

### DFS vs BFS

DFS는 좋은 선택이 아닐 수도 있다. 그래프의 깊이가 너무 정보일지 가늠하기 어렵고, spider trap이 있는 경우 무한 루프에 빠질 수도 있다. (spider trap의 경우 url의 최대 길이를 제한하거나, 사람이 수작업으로 Url 필터 목록에 걸어둬야 한다.)그래서 보통 BFS를 사용하나, 두 가지 문제점이 있다. 한 페이지에서 같은 사이트로 되돌아가는 url이 많을 때 크롤링 작업을 병렬로 처리할 경우 수많은 요청으로 인해 서버 입장에서 Dos 공격으로 간주할 수 있다. 또한 일반적인 BFS 알고리즘은 우선순위를 두지 않는다. 그러나 웹페이지는 Page rank, 사용자의 트래픽의 양, 업데이트 빈도에 따라 우선순위를 구별해야 한다. 

![image](https://github.com/junchanpp/2023-system-design-interview-2nd/assets/49396352/58506666-a90c-4a2f-ac14-798f103d7e6a)


첫 번째 문제점을 해결하기 위해서 큐 라우터를 통해 각 사이트별로 별도의 큐를 사용한다. 이를 통해 동일 웹사이트에 대해 한 번에 하나의 요청만 하게 한다. 또한 시간차를 두고 실행하도록 하면 짧은 시간 내에 너무 많은 요청을 보내는 것을 방지할 수 있다.

두 번째 문제점을 해결하기 위해서 순위결정장치를 도입한다. 순위결정장치로 우선순위를 계산하고, 우선순위별로 큐가 하나씩 입력된다. 우선순위가 높을 수록 큐 선택기에 의해 선택될 확률도 올라가게 한다.

그림을 보면 전면 큐는 우선순위 결정 과정을 처리하고, 후면 큐는 크롤러가 예의 바르게 동작하도록 보장하게 해준다.
